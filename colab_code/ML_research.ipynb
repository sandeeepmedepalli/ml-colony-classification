{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeeepmedepalli/ml-colony-classification/blob/main/ML_research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CFcTKeY7t3-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0653224f-09eb-4983-97bd-62d1eb4fc39b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os, random, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZf5K2giu6z4",
        "outputId": "2d643dd6-09b6-464a-b26e-3afdec981ede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "CSV_PATH:   /content/drive/MyDrive/22022540/annot_tab.csv\n",
            "IMAGES_DIR: /content/drive/MyDrive/22022540\n"
          ]
        }
      ],
      "source": [
        "# Your Drive folder (contains BOTH images/ and the CSV)\n",
        "BASE_DIR  = \"/content/drive/MyDrive/22022540\"\n",
        "\n",
        "IMAGES_DIR = BASE_DIR                     #  images are directly here\n",
        "CSV_PATH   = os.path.join(BASE_DIR, \"annot_tab.csv\")   # change name if your csv name is different\n",
        "\n",
        "# Output workspace (temporary, inside Colab runtime)\n",
        "WORKDIR = \"/content/colony_stage2\"\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "\n",
        "# Patch settings\n",
        "PATCH_SIZE = 100\n",
        "PAD_TO_SQUARE = True\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "print(\"CSV_PATH:  \", CSV_PATH)\n",
        "print(\"IMAGES_DIR:\", IMAGES_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdYj1vKSe-IY",
        "outputId": "8a2c6586-5df0-42c4-a618-ab3c5b86531a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using local images dir: /content/colony_images\n"
          ]
        }
      ],
      "source": [
        "#Copy images from Drive → local Colab\n",
        "import shutil, os\n",
        "\n",
        "LOCAL_IMG_DIR = \"/content/colony_images\"\n",
        "os.makedirs(LOCAL_IMG_DIR, exist_ok=True)\n",
        "\n",
        "# copy only once\n",
        "for f in os.listdir(IMAGES_DIR):\n",
        "    src = os.path.join(IMAGES_DIR, f)\n",
        "    dst = os.path.join(LOCAL_IMG_DIR, f)\n",
        "    if os.path.isfile(src) and not os.path.exists(dst): #if it is file and does not exist destination , then it copy that file\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "IMAGES_DIR = LOCAL_IMG_DIR   # changed the image directory from drive to local colab\n",
        "print(\"Now using local images dir:\", IMAGES_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ5Ej62FvJPV",
        "outputId": "195b7131-bfd3-4f92-f0c3-c5de06d67635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows (boxes): 56862\n",
            "Unique classes: 24\n",
            "label_name\n",
            "sp21    11160\n",
            "sp23     7067\n",
            "sp22     6814\n",
            "sp06     5513\n",
            "sp10     4364\n",
            "sp05     4102\n",
            "sp19     2782\n",
            "sp13     1799\n",
            "sp09     1775\n",
            "sp02     1530\n",
            "sp18     1383\n",
            "sp16     1348\n",
            "sp14     1102\n",
            "sp07     1087\n",
            "sp15      866\n",
            "sp20      853\n",
            "sp24      787\n",
            "sp11      481\n",
            "sp12      461\n",
            "sp01      397\n",
            "sp08      368\n",
            "sp04      304\n",
            "sp03      295\n",
            "sp17      224\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "required_cols = [\n",
        "    \"label_name\",\"bbox_x\",\"bbox_y\",\"bbox_width\",\"bbox_height\",\n",
        "    \"image_name\",\"image_width\",\"image_height\"\n",
        "]\n",
        "\n",
        "df = pd.read_csv(CSV_PATH, encoding=\"utf-8-sig\")\n",
        "\n",
        "missing = [c for c in required_cols if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"CSV missing columns: {missing}\\nFound: {list(df.columns)}\")\n",
        "\n",
        "# Make numeric columns numeric\n",
        "for c in [\"bbox_x\",\"bbox_y\",\"bbox_width\",\"bbox_height\",\"image_width\",\"image_height\"]:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Drop bad rows\n",
        "df = df.dropna(subset=required_cols).copy()\n",
        "df = df[(df[\"bbox_width\"] > 0) & (df[\"bbox_height\"] > 0)].copy()\n",
        "\n",
        "print(\"Rows (boxes):\", len(df))\n",
        "print(\"Unique classes:\", df[\"label_name\"].nunique())\n",
        "print(df[\"label_name\"].value_counts().head(24))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU-mH2KExCk5",
        "outputId": "d9161272-ad50-40ce-a5b5-a3ced237be45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10-class counts:\n",
            "  sp02: 1530\n",
            "  sp05: 4102\n",
            "  sp06: 5513\n",
            "  sp07: 1087\n",
            "  sp10: 4364\n",
            "  sp14: 1102\n",
            "  sp16: 1348\n",
            "  sp19: 2782\n",
            "  sp21: 11160\n",
            "  sp23: 7067\n",
            "\n",
            "4-class counts:\n",
            "  sp05: 4102\n",
            "  sp10: 4364\n",
            "  sp22: 6814\n",
            "  sp13: 1799\n"
          ]
        }
      ],
      "source": [
        "# 10-class list\n",
        "CLASSES_10 = [\"sp02\",\"sp05\",\"sp06\",\"sp07\",\"sp10\",\"sp14\",\"sp16\",\"sp19\",\"sp21\",\"sp23\"]\n",
        "\n",
        "# 4-class list:\n",
        "# - choose 2 overlap from CLASSES_10 that have >=2000 (so we can take \"fresh 1000\" later)\n",
        "# - choose 2 new classes NOT in CLASSES_10, each >=1000\n",
        "CLASSES_4_OVERLAP = [\"sp05\", \"sp10\"]     # both have >2000 boxes in your CSV\n",
        "CLASSES_4_NEW     = [\"sp22\", \"sp13\"]     # both >1000 and NOT in CLASSES_10\n",
        "CLASSES_4 = CLASSES_4_OVERLAP + CLASSES_4_NEW\n",
        "\n",
        "# Count check\n",
        "counts = df[\"label_name\"].value_counts()\n",
        "def show_counts(class_list, name):\n",
        "    print(f\"\\n{name} counts:\")\n",
        "    for c in class_list:\n",
        "        print(f\"  {c}: {int(counts.get(c, 0))}\")\n",
        "\n",
        "show_counts(CLASSES_10, \"10-class\")\n",
        "show_counts(CLASSES_4,  \"4-class\")\n",
        "\n",
        "# Safety checks for classes count to be greater than 1000 , as we are working on thousand\n",
        "for c in CLASSES_10:\n",
        "    if counts.get(c, 0) < 1000:\n",
        "        raise ValueError(f\"{c} has < 1000 boxes. Pick another class.\")\n",
        "\n",
        "for c in CLASSES_4_NEW:\n",
        "    if counts.get(c, 0) < 1000:\n",
        "        raise ValueError(f\"{c} has < 1000 boxes. Pick another NEW class.\")\n",
        "\n",
        "for c in CLASSES_4_OVERLAP:\n",
        "    if counts.get(c, 0) < 2000:\n",
        "        raise ValueError(f\"{c} overlap class must have >=2000 boxes to allow 'fresh 1000'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsx2vgPJ1sYa",
        "outputId": "9eaa7051-9490-4d73-ab54-2fd3abae0f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrain rows: 10000  expected: 10000\n",
            "Fine-tune rows: 4000  expected: 4000\n"
          ]
        }
      ],
      "source": [
        "def sample_rows_for_class(df, cls, n, seed, exclude_index_set=None):\n",
        "    \"\"\"Sample n rows for one class. Optionally exclude some row indices.\"\"\"\n",
        "    sub = df[df[\"label_name\"] == cls]\n",
        "    if exclude_index_set is not None:\n",
        "        sub = sub[~sub.index.isin(exclude_index_set)]\n",
        "    if len(sub) < n:\n",
        "        raise ValueError(f\"Not enough rows for {cls}: need {n}, have {len(sub)} after exclusions.\")\n",
        "    return sub.sample(n=n, random_state=seed)\n",
        "\n",
        "# 10-class pretrain sampling: 1000 per class\n",
        "pretrain_parts = []\n",
        "used_idx = set()\n",
        "\n",
        "for i, cls in enumerate(CLASSES_10):\n",
        "    samp = sample_rows_for_class(df, cls, n=1000, seed=SEED+i)\n",
        "    pretrain_parts.append(samp)\n",
        "    used_idx.update(samp.index.tolist())\n",
        "\n",
        "df_pretrain = pd.concat(pretrain_parts).reset_index(drop=True)\n",
        "print(\"Pretrain rows:\", len(df_pretrain), \" expected:\", 10*1000)\n",
        "\n",
        "# 4-class fine-tune sampling: 1000 per class\n",
        "# For overlap classes, we MUST avoid reusing the same boxes used above\n",
        "finetune_parts = []\n",
        "\n",
        "for j, cls in enumerate(CLASSES_4):\n",
        "    exclude = used_idx if cls in CLASSES_4_OVERLAP else None\n",
        "    samp = sample_rows_for_class(df, cls, n=1000, seed=SEED+100+j, exclude_index_set=exclude)\n",
        "    finetune_parts.append(samp)\n",
        "\n",
        "df_finetune = pd.concat(finetune_parts).reset_index(drop=True)\n",
        "print(\"Fine-tune rows:\", len(df_finetune), \" expected:\", 4*1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JS5xIUfb3kC5"
      },
      "outputs": [],
      "source": [
        "# Cropping function where we convert bbox to patch (with optional context expansion)\n",
        "def crop_patch(\n",
        "    img: Image.Image,\n",
        "    x, y, w, h,\n",
        "    pad_to_square=True,\n",
        "    expand_factor=0.20,    # e.g., 0.20 means 20% of bbox size added on each side\n",
        "    expand_px=0           # optional fixed extra pixels on each side\n",
        "):\n",
        "    # Expand bbox to include context\n",
        "    dx = expand_px + expand_factor * float(w)\n",
        "    dy = expand_px + expand_factor * float(h)\n",
        "\n",
        "    x1 = int(round(x - dx))\n",
        "    y1 = int(round(y - dy))\n",
        "    x2 = int(round(x + w + dx))\n",
        "    y2 = int(round(y + h + dy))\n",
        "\n",
        "    # Clip to image boundaries\n",
        "    x1 = max(0, x1); y1 = max(0, y1)\n",
        "    x2 = min(img.width, x2); y2 = min(img.height, y2)\n",
        "\n",
        "    # Safety: avoid empty crops\n",
        "    if x2 <= x1: x2 = min(img.width, x1 + 1)\n",
        "    if y2 <= y1: y2 = min(img.height, y1 + 1)\n",
        "\n",
        "    patch = img.crop((x1, y1, x2, y2))\n",
        "\n",
        "    if not pad_to_square:\n",
        "        return patch\n",
        "\n",
        "    # Pad to square (keeps aspect ratio before resize)\n",
        "    side = max(patch.width, patch.height)\n",
        "    new_img = Image.new(\"L\", (side, side), color=0)  # grayscale canvas\n",
        "    px = (side - patch.width) // 2\n",
        "    py = (side - patch.height) // 2\n",
        "    patch = patch.convert(\"L\")\n",
        "    new_img.paste(patch, (px, py))\n",
        "    return new_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f0KG4zfs4EPi"
      },
      "outputs": [],
      "source": [
        "class ColonyPatchDatasetCached(Dataset):\n",
        "    def __init__(self, df_rows, images_dir, class_to_idx, transform=None):\n",
        "        self.df = df_rows.reset_index(drop=True)\n",
        "        self.images_dir = images_dir\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "        self.cache = {}  # idx to PIL RGB patch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx not in self.cache:\n",
        "            r = self.df.iloc[idx]\n",
        "            img_path = os.path.join(self.images_dir, os.path.basename(r[\"image_name\"]))\n",
        "            img = Image.open(img_path).convert(\"L\")  # image is converted to gray scale\n",
        "\n",
        "            patch = crop_patch(   # crop patch and resize it to 100*100\n",
        "                img, r[\"bbox_x\"], r[\"bbox_y\"], r[\"bbox_width\"], r[\"bbox_height\"],\n",
        "                pad_to_square=PAD_TO_SQUARE\n",
        "            ).resize((PATCH_SIZE, PATCH_SIZE), resample=Image.BILINEAR)\n",
        "\n",
        "            patch_rgb = Image.merge(\"RGB\", (patch, patch, patch))\n",
        "            self.cache[idx] = patch_rgb\n",
        "\n",
        "        patch_rgb = self.cache[idx]\n",
        "        y = self.class_to_idx[str(self.df.iloc[idx][\"label_name\"])]\n",
        "\n",
        "        if self.transform:\n",
        "            patch_rgb = self.transform(patch_rgb)\n",
        "\n",
        "        return patch_rgb, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aVbqNKUX4T35"
      },
      "outputs": [],
      "source": [
        "# ImageNet normalization stats (because we use an ImageNet-pretrained backbone)\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_tf = T.Compose([\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomVerticalFlip(),\n",
        "    T.RandomRotation(degrees=10),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "test_tf = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c2WwKm_Z4io7"
      },
      "outputs": [],
      "source": [
        "def accuracy(pred_logits, y):\n",
        "    preds = pred_logits.argmax(dim=1)\n",
        "    return (preds == y).float().mean().item()\n",
        "\n",
        "def train_one_epoch(model, loader, optim, criterion):\n",
        "    model.train()\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy(logits, y) * bs\n",
        "        n += bs\n",
        "\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "@torch.no_grad()  # helps to stop gradient tracking during evaluation\n",
        "def eval_one_epoch(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        bs = x.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy(logits, y) * bs\n",
        "        n += bs\n",
        "\n",
        "    return total_loss / n, total_acc / n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIdWtwdS43PF",
        "outputId": "cbf88edf-f593-46d3-fd91-91aa11a5dce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-class train: 8000 test: 2000\n"
          ]
        }
      ],
      "source": [
        "class_to_idx_10 = {c:i for i,c in enumerate(CLASSES_10)}\n",
        "\n",
        "train_df_10, test_df_10 = train_test_split(\n",
        "    df_pretrain,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    stratify=df_pretrain[\"label_name\"]\n",
        ")\n",
        "\n",
        "ds_train_10 = ColonyPatchDatasetCached(train_df_10, IMAGES_DIR, class_to_idx_10, transform=train_tf)\n",
        "ds_test_10  = ColonyPatchDatasetCached(test_df_10,  IMAGES_DIR, class_to_idx_10, transform=test_tf)\n",
        "\n",
        "train_loader_10 = DataLoader(ds_train_10, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
        "test_loader_10  = DataLoader(ds_test_10,  batch_size=128, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "\n",
        "print(\"10-class train:\", len(ds_train_10), \"test:\", len(ds_test_10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x7bXe8S5547y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa60fdb-d641-4a35-f89f-d69fc200736a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 184MB/s]\n"
          ]
        }
      ],
      "source": [
        "#we are going to use the pretrained back bone for imagenet weights\n",
        "#ResNet-18 architecture suitable for efficient transfer learning.\n",
        "#Load the official pretrained ResNet18 weights trained on ImageNet-1K(1000 classes), version 1\n",
        "weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
        "model_10 = models.resnet18(weights=weights)\n",
        "\n",
        "# change the resnet classifier head from 1000 to 10 classes\n",
        "in_features = model_10.fc.in_features\n",
        "model_10.fc = nn.Linear(in_features, len(CLASSES_10))\n",
        "\n",
        "model_10 = model_10.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_10.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abTl3Ece6Iem",
        "outputId": "0021fed8-bcf0-4f2e-a51c-012f076f9f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10-class] Epoch 01 | train acc 0.666 | test acc 0.816\n",
            "[10-class] Epoch 02 | train acc 0.865 | test acc 0.878\n",
            "[10-class] Epoch 03 | train acc 0.901 | test acc 0.881\n",
            "[10-class] Epoch 04 | train acc 0.914 | test acc 0.903\n",
            "[10-class] Epoch 05 | train acc 0.925 | test acc 0.904\n",
            "[10-class] Epoch 06 | train acc 0.936 | test acc 0.911\n",
            "[10-class] Epoch 07 | train acc 0.943 | test acc 0.918\n",
            "[10-class] Epoch 08 | train acc 0.942 | test acc 0.922\n",
            "[10-class] Epoch 09 | train acc 0.956 | test acc 0.911\n",
            "[10-class] Epoch 10 | train acc 0.956 | test acc 0.894\n",
            "Best 10-class test acc: 0.9215000014305115\n"
          ]
        }
      ],
      "source": [
        "EPOCHS_10 = 10  # start small; you can increase later\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(1, EPOCHS_10+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model_10, train_loader_10, optimizer, criterion)\n",
        "    te_loss, te_acc = eval_one_epoch(model_10, test_loader_10, criterion)\n",
        "\n",
        "    print(f\"[10-class] Epoch {epoch:02d} | train acc {tr_acc:.3f} | test acc {te_acc:.3f}\")\n",
        "\n",
        "    #save the model with best accuracy\n",
        "    if te_acc > best_acc:\n",
        "        best_acc = te_acc\n",
        "        torch.save(model_10.state_dict(), os.path.join(WORKDIR, \"resnet18_pretrained_10class.pt\"))\n",
        "\n",
        "print(\"Best 10-class test acc:\", best_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, dataloader, class_names, device, title=\"\"):\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    for x, y in dataloader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        logits = model(x)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_targets.append(y.cpu().numpy())\n",
        "\n",
        "    y_pred = np.concatenate(all_preds)\n",
        "    y_true = np.concatenate(all_targets)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(title)\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print()\n",
        "\n",
        "    rep = classification_report(y_true, y_pred, target_names=class_names, digits=3)\n",
        "    print(\"Classification Report:\")\n",
        "    print(rep)\n",
        "\n",
        "    return cm, rep"
      ],
      "metadata": {
        "id": "9uANsT_cV4Z7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 1) Evaluate BEST 10-class PUBLIC model (ResNet18)\n",
        "# =========================================================\n",
        "path_10 = os.path.join(WORKDIR, \"resnet18_pretrained_10class.pt\")  # <-- this matches YOUR training save\n",
        "\n",
        "model_10_eval = models.resnet18(weights=None)\n",
        "model_10_eval.fc = nn.Linear(model_10_eval.fc.in_features, len(CLASSES_10))\n",
        "model_10_eval.load_state_dict(torch.load(path_10, map_location=DEVICE))\n",
        "model_10_eval = model_10_eval.to(DEVICE)\n",
        "\n",
        "cm_10, report_10 = evaluate_model(\n",
        "    model_10_eval,\n",
        "    test_loader_10,\n",
        "    CLASSES_10,\n",
        "    DEVICE,\n",
        "    title=\"Evaluation: 10-Class PUBLIC (Best Saved Model)\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTGUegjiWCqQ",
        "outputId": "c63cb1b7-7f13-4f1b-c339-b19016aaa717"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Evaluation: 10-Class PUBLIC (Best Saved Model)\n",
            "======================================================================\n",
            "Confusion Matrix:\n",
            "[[192   2   0   0   0   0   1   0   4   1]\n",
            " [  3 191   0   2   0   2   0   0   1   1]\n",
            " [  0   0 176   1   2  10   0   3   3   5]\n",
            " [  0   0   4 185   2   1   0   2   1   5]\n",
            " [  0   0   8   2 177   3   0   2   0   8]\n",
            " [  0   0   5   0   0 192   0   3   0   0]\n",
            " [  0   0   2   0   1   1 179   8   6   3]\n",
            " [  1   0   1   0   0   2   0 195   1   0]\n",
            " [  1   0   1   1   0   0   0   0 196   1]\n",
            " [  3   2  23   3   3   2   4   0   0 160]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        sp02      0.960     0.960     0.960       200\n",
            "        sp05      0.979     0.955     0.967       200\n",
            "        sp06      0.800     0.880     0.838       200\n",
            "        sp07      0.954     0.925     0.939       200\n",
            "        sp10      0.957     0.885     0.919       200\n",
            "        sp14      0.901     0.960     0.930       200\n",
            "        sp16      0.973     0.895     0.932       200\n",
            "        sp19      0.915     0.975     0.944       200\n",
            "        sp21      0.925     0.980     0.951       200\n",
            "        sp23      0.870     0.800     0.833       200\n",
            "\n",
            "    accuracy                          0.921      2000\n",
            "   macro avg      0.923     0.922     0.921      2000\n",
            "weighted avg      0.923     0.921     0.921      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNsGoSG-a-Sc",
        "outputId": "90e1dddc-4486-4d75-8cef-7ffc2b122b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4-class train: 3200  expected: 3200\n",
            "4-class test:  800  expected: 800\n",
            "label_name\n",
            "sp05    800\n",
            "sp10    800\n",
            "sp22    800\n",
            "sp13    800\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "class_to_idx_4 = {c:i for i,c in enumerate(CLASSES_4)}\n",
        "\n",
        "train_parts = []\n",
        "test_parts = []\n",
        "\n",
        "for cls in CLASSES_4:\n",
        "    sub = df_finetune[df_finetune[\"label_name\"] == cls].sample(frac=1.0, random_state=SEED)  # shuffle\n",
        "    train_parts.append(sub.iloc[:800])\n",
        "    test_parts.append(sub.iloc[800:1000])\n",
        "\n",
        "train_df_4 = pd.concat(train_parts).reset_index(drop=True)\n",
        "test_df_4  = pd.concat(test_parts).reset_index(drop=True)\n",
        "\n",
        "print(\"4-class train:\", len(train_df_4), \" expected:\", 4*800)\n",
        "print(\"4-class test: \", len(test_df_4),  \" expected:\", 4*200)\n",
        "print(train_df_4[\"label_name\"].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xfgOZRHLbknR"
      },
      "outputs": [],
      "source": [
        "ds_train_4 = ColonyPatchDatasetCached(train_df_4, IMAGES_DIR, class_to_idx_4, transform=train_tf)\n",
        "ds_test_4  = ColonyPatchDatasetCached(test_df_4,  IMAGES_DIR, class_to_idx_4, transform=test_tf)\n",
        "\n",
        "train_loader_4 = DataLoader(ds_train_4, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
        "test_loader_4  = DataLoader(ds_test_4,  batch_size=128, shuffle=False, num_workers=0, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iN4i72DSbR6z"
      },
      "outputs": [],
      "source": [
        "# Start from the best 10-class checkpoint\n",
        "model_ft = models.resnet18(weights=None)    # fresh resnet , without pretrained weights\n",
        "in_features = model_ft.fc.in_features   # helps in rebuilding the final layer\n",
        "model_ft.fc = nn.Linear(in_features, len(CLASSES_10))  # temporary to load weights\n",
        "model_ft.load_state_dict(torch.load(os.path.join(WORKDIR, \"resnet18_pretrained_10class.pt\"), map_location=\"cpu\"))\n",
        "\n",
        "# Freeze everything (feature extractor)\n",
        "for p in model_ft.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Replace final layer with a NEW 4-class head\n",
        "model_ft.fc = nn.Linear(in_features, len(CLASSES_4))\n",
        "\n",
        "# Only the head will train\n",
        "for p in model_ft.fc.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "model_ft = model_ft.to(DEVICE)\n",
        "\n",
        "criterion_ft = nn.CrossEntropyLoss()\n",
        "optimizer_ft = torch.optim.Adam(model_ft.fc.parameters(), lr=1e-3)  # a bit higher since only head trains\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhwwEofdbWHh",
        "outputId": "89047471-1afe-4aae-e3b4-2926d4dc0afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4-class FT] Epoch 01 | train acc 0.748 | test acc 0.895\n",
            "[4-class FT] Epoch 02 | train acc 0.907 | test acc 0.917\n",
            "[4-class FT] Epoch 03 | train acc 0.919 | test acc 0.930\n",
            "[4-class FT] Epoch 04 | train acc 0.922 | test acc 0.934\n",
            "[4-class FT] Epoch 05 | train acc 0.931 | test acc 0.936\n",
            "[4-class FT] Epoch 06 | train acc 0.931 | test acc 0.936\n",
            "[4-class FT] Epoch 07 | train acc 0.938 | test acc 0.941\n",
            "[4-class FT] Epoch 08 | train acc 0.941 | test acc 0.940\n",
            "[4-class FT] Epoch 09 | train acc 0.938 | test acc 0.945\n",
            "[4-class FT] Epoch 10 | train acc 0.941 | test acc 0.940\n",
            "Best 4-class test acc: 0.945\n",
            "4 classes: ['sp05', 'sp10', 'sp22', 'sp13']\n"
          ]
        }
      ],
      "source": [
        "EPOCHS_4 = 10\n",
        "\n",
        "best_acc_4 = 0.0\n",
        "for epoch in range(1, EPOCHS_4+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model_ft, train_loader_4, optimizer_ft, criterion_ft)\n",
        "    te_loss, te_acc = eval_one_epoch(model_ft, test_loader_4, criterion_ft)\n",
        "\n",
        "    print(f\"[4-class FT] Epoch {epoch:02d} | train acc {tr_acc:.3f} | test acc {te_acc:.3f}\")\n",
        "\n",
        "    if te_acc > best_acc_4:\n",
        "        best_acc_4 = te_acc\n",
        "        torch.save(model_ft.state_dict(), os.path.join(WORKDIR, \"resnet18_finetuned_4class.pt\"))\n",
        "\n",
        "print(\"Best 4-class test acc:\", best_acc_4)\n",
        "print(\"4 classes:\", CLASSES_4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Full fine-tuning (unfreeze backbone + keep head)\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Unfreeze ALL parameters (backbone + head)\n",
        "for p in model_ft.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Build optimizer for full fine-tuning\n",
        "# smaller LR for backbone, bigger LR for head\n",
        "backbone_params = []\n",
        "head_params = []\n",
        "for name, p in model_ft.named_parameters():\n",
        "    if not p.requires_grad:\n",
        "        continue\n",
        "    if name.startswith(\"fc.\"):\n",
        "        head_params.append(p)\n",
        "    else:\n",
        "        backbone_params.append(p)\n",
        "\n",
        "optimizer_full = torch.optim.Adam([\n",
        "    {\"params\": backbone_params, \"lr\": 1e-5},  # backbone learns slowly\n",
        "    {\"params\": head_params,     \"lr\": 1e-4},  # head learns faster\n",
        "])\n",
        "\n",
        "# Continue training and track train/test accuracy\n",
        "EPOCHS_FULL = 10\n",
        "best_acc_full = 0.0\n",
        "best_path = os.path.join(WORKDIR, \"resnet18_finetuned_4class_full.pt\")\n",
        "\n",
        "for epoch in range(1, EPOCHS_FULL + 1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model_ft, train_loader_4, optimizer_full, criterion_ft)\n",
        "    te_loss, te_acc = eval_one_epoch(model_ft, test_loader_4, criterion_ft)\n",
        "\n",
        "    print(f\"[4-class FULL FT] Epoch {epoch:02d} | train acc {tr_acc:.3f} | test acc {te_acc:.3f}\")\n",
        "\n",
        "    # Save best model by test accuracy\n",
        "    if te_acc > best_acc_full:\n",
        "        best_acc_full = te_acc\n",
        "        torch.save(model_ft.state_dict(), best_path)\n",
        "\n",
        "print(\"Best 4-class test acc (full fine-tune):\", best_acc_full)\n",
        "print(\"Saved best full-finetuned model to:\", best_path)\n"
      ],
      "metadata": {
        "id": "YXAAuSWez2ER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc3a92f-5755-4af1-d8e3-b22f3da07493"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4-class FULL FT] Epoch 01 | train acc 0.950 | test acc 0.943\n",
            "[4-class FULL FT] Epoch 02 | train acc 0.961 | test acc 0.951\n",
            "[4-class FULL FT] Epoch 03 | train acc 0.970 | test acc 0.951\n",
            "[4-class FULL FT] Epoch 04 | train acc 0.971 | test acc 0.956\n",
            "[4-class FULL FT] Epoch 05 | train acc 0.975 | test acc 0.959\n",
            "[4-class FULL FT] Epoch 06 | train acc 0.973 | test acc 0.958\n",
            "[4-class FULL FT] Epoch 07 | train acc 0.978 | test acc 0.958\n",
            "[4-class FULL FT] Epoch 08 | train acc 0.978 | test acc 0.959\n",
            "[4-class FULL FT] Epoch 09 | train acc 0.983 | test acc 0.963\n",
            "[4-class FULL FT] Epoch 10 | train acc 0.981 | test acc 0.963\n",
            "Best 4-class test acc (full fine-tune): 0.9625\n",
            "Saved best full-finetuned model to: /content/colony_stage2/resnet18_finetuned_4class_full.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# 2) Evaluate BEST 4-class FULL fine-tuned model (ResNet18)\n",
        "# =========================================================\n",
        "path_4 = os.path.join(WORKDIR, \"resnet18_finetuned_4class_full.pt\")  # <-- update if your filename differs\n",
        "\n",
        "model_4_eval = models.resnet18(weights=None)\n",
        "\n",
        "# IMPORTANT:\n",
        "# your checkpoint was trained with a 4-class head (after fine-tuning),\n",
        "# so we rebuild ResNet with 4 outputs.\n",
        "model_4_eval.fc = nn.Linear(model_4_eval.fc.in_features, len(CLASSES_4))\n",
        "\n",
        "model_4_eval.load_state_dict(torch.load(path_4, map_location=DEVICE))\n",
        "model_4_eval = model_4_eval.to(DEVICE)\n",
        "\n",
        "cm_4, report_4 = evaluate_model(\n",
        "    model_4_eval,\n",
        "    test_loader_4,\n",
        "    CLASSES_4,\n",
        "    DEVICE,\n",
        "    title=\"Evaluation: 4-Class PUBLIC Fine-Tuned (Best Saved Model)\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWr7iBPz2NZI",
        "outputId": "e3600aae-98a8-4b57-af14-0460349b1a75"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "Evaluation: 4-Class PUBLIC Fine-Tuned (Best Saved Model)\n",
            "======================================================================\n",
            "Confusion Matrix:\n",
            "[[197   1   0   2]\n",
            " [  2 185  13   0]\n",
            " [  0   3 196   1]\n",
            " [  3   0   5 192]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        sp05      0.975     0.985     0.980       200\n",
            "        sp10      0.979     0.925     0.951       200\n",
            "        sp22      0.916     0.980     0.947       200\n",
            "        sp13      0.985     0.960     0.972       200\n",
            "\n",
            "    accuracy                          0.963       800\n",
            "   macro avg      0.964     0.963     0.963       800\n",
            "weighted avg      0.964     0.963     0.963       800\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# ============================================================\n",
        "# 1) Confusion Matrix PNG (Blue/White like PDF)\n",
        "# ============================================================\n",
        "def save_cm_png(cm, class_names, out_path, title=\"Confusion Matrix\", normalize=False, show_colorbar=True):\n",
        "    \"\"\"\n",
        "    Paper-style Confusion Matrix (PDF-like):\n",
        "    - Blue/white colormap (Blues)\n",
        "    - Bigger fonts\n",
        "    - Thicker gridlines (black)\n",
        "    - Square cells\n",
        "    - Clean layout\n",
        "    \"\"\"\n",
        "    cm = np.array(cm, dtype=float)\n",
        "\n",
        "    if normalize:\n",
        "        row_sums = cm.sum(axis=1, keepdims=True) + 1e-9\n",
        "        cm_show = cm / row_sums\n",
        "        vmin, vmax = 0.0, 1.0\n",
        "    else:\n",
        "        cm_show = cm\n",
        "        vmin, vmax = 0.0, float(cm_show.max()) if cm_show.size else 1.0\n",
        "\n",
        "    # ✅ Bigger figure + better paper layout\n",
        "    fig, ax = plt.subplots(figsize=(11, 9))\n",
        "    ax.set_facecolor(\"white\")\n",
        "\n",
        "    # ✅ Blue-white look like the PDF\n",
        "    im = ax.imshow(cm_show, interpolation=\"nearest\", cmap=\"Blues\", vmin=vmin, vmax=vmax)\n",
        "\n",
        "    # ✅ Title and labels with bigger fonts\n",
        "    ax.set_title(title, fontsize=18, fontweight=\"bold\", pad=16)\n",
        "    ax.set_xlabel(\"Predicted label\", fontsize=16, fontweight=\"bold\", labelpad=10)\n",
        "    ax.set_ylabel(\"True label\", fontsize=16, fontweight=\"bold\", labelpad=10)\n",
        "\n",
        "    # ✅ Ticks with bigger fonts\n",
        "    ticks = np.arange(len(class_names))\n",
        "    ax.set_xticks(ticks)\n",
        "    ax.set_yticks(ticks)\n",
        "    ax.set_xticklabels(class_names, rotation=45, ha=\"right\", fontsize=14)\n",
        "    ax.set_yticklabels(class_names, fontsize=14)\n",
        "\n",
        "    # ✅ Optional colorbar (papers sometimes keep it, sometimes not)\n",
        "    if show_colorbar:\n",
        "        cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "        cbar.ax.tick_params(labelsize=12)\n",
        "\n",
        "    # ✅ Thick gridlines like PDF\n",
        "    ax.set_xticks(np.arange(-.5, len(class_names), 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-.5, len(class_names), 1), minor=True)\n",
        "    ax.grid(which=\"minor\", color=\"black\", linestyle=\"-\", linewidth=1.5)\n",
        "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
        "\n",
        "    # ✅ Annotate cells with larger text\n",
        "    # threshold is based on displayed range\n",
        "    thresh = (vmax - vmin) * 0.6 + vmin\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            if normalize:\n",
        "                txt = f\"{cm_show[i, j]*100:.1f}%\\n({int(cm[i, j])})\"\n",
        "            else:\n",
        "                txt = str(int(cm[i, j]))\n",
        "\n",
        "            ax.text(\n",
        "                j, i, txt,\n",
        "                ha=\"center\", va=\"center\",\n",
        "                fontsize=13, fontweight=\"bold\",\n",
        "                color=\"white\" if cm_show[i, j] > thresh else \"black\"\n",
        "            )\n",
        "\n",
        "    ax.set_aspect(\"equal\")  # ✅ square cells\n",
        "    fig.tight_layout()\n",
        "\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(\"✅ Saved CM PNG (paper-style):\", out_path)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) Classification Report -> Excel (.xlsx)\n",
        "#    Works even if report_* is a STRING (sklearn text report)\n",
        "# ============================================================\n",
        "def _parse_classification_report_string(report_str: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Parses sklearn.metrics.classification_report (text) into a DataFrame.\n",
        "    Handles typical sklearn formatting.\n",
        "    \"\"\"\n",
        "    lines = [ln.strip() for ln in str(report_str).splitlines() if ln.strip()]\n",
        "    rows = []\n",
        "\n",
        "    # Example lines look like:\n",
        "    # classA   0.95   0.90   0.92   50\n",
        "    # accuracy 0.93   200\n",
        "    # macro avg 0.94  0.93  0.93  200\n",
        "    # weighted avg ...\n",
        "\n",
        "    for ln in lines:\n",
        "        # skip header line if present\n",
        "        if ln.lower().startswith(\"precision\") and \"recall\" in ln.lower():\n",
        "            continue\n",
        "\n",
        "        parts = re.split(r\"\\s+\", ln)\n",
        "        if len(parts) < 2:\n",
        "            continue\n",
        "\n",
        "        # accuracy line is special: \"accuracy 0.932 200\"\n",
        "        if parts[0].lower() == \"accuracy\":\n",
        "            # Sometimes: accuracy 0.93 200\n",
        "            if len(parts) >= 3:\n",
        "                rows.append({\n",
        "                    \"label\": \"accuracy\",\n",
        "                    \"precision\": np.nan,\n",
        "                    \"recall\": np.nan,\n",
        "                    \"f1-score\": float(parts[1]),\n",
        "                    \"support\": float(parts[2])\n",
        "                })\n",
        "            continue\n",
        "\n",
        "        # macro avg / weighted avg are 2-word labels\n",
        "        if parts[0].lower() in [\"macro\", \"weighted\"] and len(parts) >= 6:\n",
        "            label = parts[0] + \" \" + parts[1]  # \"macro avg\"\n",
        "            precision, recall, f1, support = parts[2], parts[3], parts[4], parts[5]\n",
        "            rows.append({\n",
        "                \"label\": label,\n",
        "                \"precision\": float(precision),\n",
        "                \"recall\": float(recall),\n",
        "                \"f1-score\": float(f1),\n",
        "                \"support\": float(support)\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # normal per-class line: label + 4 numbers\n",
        "        if len(parts) >= 5:\n",
        "            label = parts[0]\n",
        "            precision, recall, f1, support = parts[1], parts[2], parts[3], parts[4]\n",
        "            # Some class names may contain spaces; if so, this parser won't catch it.\n",
        "            # In your code, class names look like \"sp01\" etc, so it's safe.\n",
        "            try:\n",
        "                rows.append({\n",
        "                    \"label\": label,\n",
        "                    \"precision\": float(precision),\n",
        "                    \"recall\": float(recall),\n",
        "                    \"f1-score\": float(f1),\n",
        "                    \"support\": float(support)\n",
        "                })\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n",
        "def save_report_excel(report_obj, out_xlsx_path, sheet_name=\"Report\"):\n",
        "    \"\"\"\n",
        "    Saves classification report to an Excel file.\n",
        "    - report_obj can be either:\n",
        "        (a) sklearn text report string, or\n",
        "        (b) a DataFrame already\n",
        "    \"\"\"\n",
        "    if isinstance(report_obj, pd.DataFrame):\n",
        "        df = report_obj.copy()\n",
        "    else:\n",
        "        df = _parse_classification_report_string(report_obj)\n",
        "\n",
        "    os.makedirs(os.path.dirname(out_xlsx_path), exist_ok=True)\n",
        "\n",
        "    # Write xlsx\n",
        "    with pd.ExcelWriter(out_xlsx_path, engine=\"openpyxl\") as writer:\n",
        "        df.to_excel(writer, index=False, sheet_name=sheet_name)\n",
        "\n",
        "        # basic formatting: auto column widths\n",
        "        ws = writer.sheets[sheet_name]\n",
        "        for col in ws.columns:\n",
        "            max_len = 0\n",
        "            col_letter = col[0].column_letter\n",
        "            for cell in col:\n",
        "                try:\n",
        "                    max_len = max(max_len, len(str(cell.value)))\n",
        "                except:\n",
        "                    pass\n",
        "            ws.column_dimensions[col_letter].width = min(max_len + 2, 35)\n",
        "\n",
        "    print(\"✅ Saved report Excel:\", out_xlsx_path)\n",
        "\n",
        "# ============================================================\n",
        "# 3) OUTPUT FOLDER (uses WORKDIR if present)\n",
        "# ============================================================\n",
        "OUT_DIR = os.path.join(WORKDIR if \"WORKDIR\" in globals() else \".\", \"reports_outputs\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "print(\"✅ Outputs will be saved under:\", OUT_DIR)\n",
        "\n",
        "# ============================================================\n",
        "# 4) SAVE: 10-class (public)\n",
        "# ============================================================\n",
        "assert \"cm_10\" in globals() and \"report_10\" in globals(), \"Run the 10-class evaluation cell first (cm_10/report_10).\"\n",
        "assert \"CLASSES_10\" in globals(), \"CLASSES_10 not found.\"\n",
        "\n",
        "save_cm_png(cm_10, CLASSES_10, os.path.join(OUT_DIR, \"cm_10class_blue.png\"),\n",
        "            title=\"Confusion Matrix — Best 10-Class Public Model\", normalize=False, show_colorbar=True)\n",
        "\n",
        "save_cm_png(cm_10, CLASSES_10, os.path.join(OUT_DIR, \"cm_10class_blue_norm.png\"),\n",
        "            title=\"Confusion Matrix (Normalized) — Best 10-Class Public Model\", normalize=True, show_colorbar=True)\n",
        "\n",
        "save_report_excel(report_10, os.path.join(OUT_DIR, \"report_10class.xlsx\"), sheet_name=\"10-class report\")\n",
        "\n",
        "# ============================================================\n",
        "# 5) SAVE: 4-class (fine-tuned)\n",
        "# ============================================================\n",
        "assert \"cm_4\" in globals() and \"report_4\" in globals(), \"Run the 4-class evaluation cell first (cm_4/report_4).\"\n",
        "assert \"CLASSES_4\" in globals(), \"CLASSES_4 not found.\"\n",
        "\n",
        "save_cm_png(cm_4, CLASSES_4, os.path.join(OUT_DIR, \"cm_4class_blue.png\"),\n",
        "            title=\"Confusion Matrix — Best 4-Class Fine-Tuned Model\", normalize=False, show_colorbar=True)\n",
        "\n",
        "save_cm_png(cm_4, CLASSES_4, os.path.join(OUT_DIR, \"cm_4class_blue_norm.png\"),\n",
        "            title=\"Confusion Matrix (Normalized) — Best 4-Class Fine-Tuned Model\", normalize=True, show_colorbar=True)\n",
        "\n",
        "save_report_excel(report_4, os.path.join(OUT_DIR, \"report_4class.xlsx\"), sheet_name=\"4-class report\")\n",
        "\n",
        "print(\"\\n✅ Done. Check outputs in:\", OUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0F3P1QiC9Lj",
        "outputId": "fa410efe-3d37-4a8b-ad1f-2ca08d5af535"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Outputs will be saved under: /content/colony_stage2/reports_outputs\n",
            "✅ Saved CM PNG (paper-style): /content/colony_stage2/reports_outputs/cm_10class_blue.png\n",
            "✅ Saved CM PNG (paper-style): /content/colony_stage2/reports_outputs/cm_10class_blue_norm.png\n",
            "✅ Saved report Excel: /content/colony_stage2/reports_outputs/report_10class.xlsx\n",
            "✅ Saved CM PNG (paper-style): /content/colony_stage2/reports_outputs/cm_4class_blue.png\n",
            "✅ Saved CM PNG (paper-style): /content/colony_stage2/reports_outputs/cm_4class_blue_norm.png\n",
            "✅ Saved report Excel: /content/colony_stage2/reports_outputs/report_4class.xlsx\n",
            "\n",
            "✅ Done. Check outputs in: /content/colony_stage2/reports_outputs\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO+ioG/6tAob6XpzKr/BN9/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
