{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeeepmedepalli/ml-colony-classification/blob/main/mixing_old_and_new_enhanced_code_of_ML_research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CFcTKeY7t3-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf0f621-44b9-4cb0-c9ec-c415e3acdb6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os, random, math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZf5K2giu6z4",
        "outputId": "229c07af-9b81-4860-ce5d-9b79ec54e5c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "CSV_PATH:   /content/drive/MyDrive/22022540/annot_tab.csv\n"
          ]
        }
      ],
      "source": [
        "# ====== PATHS (separate pretrain vs ground truth) ======\n",
        "BASE_DIR = \"/content/drive/MyDrive/22022540\"\n",
        "\n",
        "# Public dataset (10-class pretrain)\n",
        "CSV_PATH   = os.path.join(BASE_DIR, \"annot_tab.csv\")\n",
        "IMAGES_DIR_PUBLIC = BASE_DIR  # images are directly here\n",
        "\n",
        "\n",
        "# ===== Dataset A (Fine-tune train) =====\n",
        "FT_GT_ZIP_PATH   = \"/content/drive/MyDrive/22022540/ground_truth/ground_truth_one/ground_truth_dataset.zip\"\n",
        "IMAGES_DIR_FT    = \"/content/drive/MyDrive/22022540/ground_truth/ground_truth_one\"\n",
        "\n",
        "# ===== Dataset B (New test dataset) =====\n",
        "TEST_GT_ZIP_PATH = \"/content/drive/MyDrive/22022540/ground_truth/ground_truth_updated/updated_groud_truth.zip\"\n",
        "IMAGES_DIR_TEST  = \"/content/drive/MyDrive/22022540/ground_truth/ground_truth_updated\"\n",
        "\n",
        "\n",
        "# Output workspaces (separate)\n",
        "WORKDIR_PRETRAIN = \"/content/colony_stage2_pretrain10\"\n",
        "WORKDIR_GT       = \"/content/colony_stage2_groundtruth\"\n",
        "os.makedirs(WORKDIR_PRETRAIN, exist_ok=True)\n",
        "os.makedirs(WORKDIR_GT, exist_ok=True)\n",
        "\n",
        "# Patch settings\n",
        "PATCH_SIZE = 100\n",
        "PAD_TO_SQUARE = True\n",
        "\n",
        "# How much extra context around the colony bbox\n",
        "BOX_EXPAND = 0.20\n",
        "\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "print(\"CSV_PATH:  \", CSV_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdYj1vKSe-IY",
        "outputId": "98a4d4f1-1e1a-4f20-fea3-1d5f685fb06e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using local images: /content/colony_images\n"
          ]
        }
      ],
      "source": [
        "#Copy images from Drive → local Colab\n",
        "import shutil, os\n",
        "\n",
        "LOCAL_IMG_DIR = \"/content/colony_images\"\n",
        "os.makedirs(LOCAL_IMG_DIR, exist_ok=True)\n",
        "\n",
        "for f in os.listdir(IMAGES_DIR_PUBLIC):\n",
        "    src = os.path.join(IMAGES_DIR_PUBLIC, f)\n",
        "    dst = os.path.join(LOCAL_IMG_DIR, f)\n",
        "    if os.path.isfile(src) and not os.path.exists(dst):\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "IMAGES_DIR_PUBLIC = LOCAL_IMG_DIR\n",
        "print(\"Now using local images:\", IMAGES_DIR_PUBLIC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ5Ej62FvJPV",
        "outputId": "0823588a-54a0-495b-c45e-866acdac75cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows (boxes): 56862\n",
            "Unique classes: 24\n",
            "label_name\n",
            "sp21    11160\n",
            "sp23     7067\n",
            "sp22     6814\n",
            "sp06     5513\n",
            "sp10     4364\n",
            "sp05     4102\n",
            "sp19     2782\n",
            "sp13     1799\n",
            "sp09     1775\n",
            "sp02     1530\n",
            "sp18     1383\n",
            "sp16     1348\n",
            "sp14     1102\n",
            "sp07     1087\n",
            "sp15      866\n",
            "sp20      853\n",
            "sp24      787\n",
            "sp11      481\n",
            "sp12      461\n",
            "sp01      397\n",
            "sp08      368\n",
            "sp04      304\n",
            "sp03      295\n",
            "sp17      224\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "required_cols = [\n",
        "    \"label_name\",\"bbox_x\",\"bbox_y\",\"bbox_width\",\"bbox_height\",\n",
        "    \"image_name\",\"image_width\",\"image_height\"\n",
        "]\n",
        "\n",
        "df = pd.read_csv(CSV_PATH, encoding=\"utf-8-sig\")\n",
        "\n",
        "missing = [c for c in required_cols if c not in df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"CSV missing columns: {missing}\\nFound: {list(df.columns)}\")\n",
        "\n",
        "# Make numeric columns numeric\n",
        "for c in [\"bbox_x\",\"bbox_y\",\"bbox_width\",\"bbox_height\",\"image_width\",\"image_height\"]:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Drop bad rows\n",
        "df = df.dropna(subset=required_cols).copy()\n",
        "df = df[(df[\"bbox_width\"] > 0) & (df[\"bbox_height\"] > 0)].copy()\n",
        "\n",
        "print(\"Rows (boxes):\", len(df))\n",
        "print(\"Unique classes:\", df[\"label_name\"].nunique())\n",
        "print(df[\"label_name\"].value_counts().head(24))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU-mH2KExCk5",
        "outputId": "8238d4db-1d4c-4805-9555-3103a8645390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10-class counts:\n",
            "  sp02: 1530\n",
            "  sp05: 4102\n",
            "  sp06: 5513\n",
            "  sp07: 1087\n",
            "  sp10: 4364\n",
            "  sp14: 1102\n",
            "  sp16: 1348\n",
            "  sp19: 2782\n",
            "  sp21: 11160\n",
            "  sp23: 7067\n"
          ]
        }
      ],
      "source": [
        "# 10-class list\n",
        "CLASSES_10 = [\"sp02\",\"sp05\",\"sp06\",\"sp07\",\"sp10\",\"sp14\",\"sp16\",\"sp19\",\"sp21\",\"sp23\"]\n",
        "\n",
        "# Count check\n",
        "counts = df[\"label_name\"].value_counts()\n",
        "def show_counts(class_list, name):\n",
        "    print(f\"\\n{name} counts:\")\n",
        "    for c in class_list:\n",
        "        print(f\"  {c}: {int(counts.get(c, 0))}\")\n",
        "\n",
        "show_counts(CLASSES_10, \"10-class\")\n",
        "\n",
        "# Safety checks for classes count to be greater than 1000 , as we are working on thousand\n",
        "for c in CLASSES_10:\n",
        "    if counts.get(c, 0) < 1000:\n",
        "        raise ValueError(f\"{c} has < 1000 boxes. Pick another class.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsx2vgPJ1sYa",
        "outputId": "8e88aa09-c877-4084-b0cd-f81f154349c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrain rows: 10000  expected: 10000\n"
          ]
        }
      ],
      "source": [
        "def sample_rows_for_class(df, cls, n, seed, exclude_index_set=None):\n",
        "    \"\"\"Sample n rows for one class. Optionally exclude some row indices.\"\"\"\n",
        "    sub = df[df[\"label_name\"] == cls]\n",
        "    if exclude_index_set is not None:\n",
        "        sub = sub[~sub.index.isin(exclude_index_set)]\n",
        "    if len(sub) < n:\n",
        "        raise ValueError(f\"Not enough rows for {cls}: need {n}, have {len(sub)} after exclusions.\")\n",
        "    return sub.sample(n=n, random_state=seed)\n",
        "\n",
        "# 10-class pretrain sampling: 1000 per class\n",
        "pretrain_parts = []\n",
        "used_idx = set()\n",
        "\n",
        "for i, cls in enumerate(CLASSES_10):\n",
        "    samp = sample_rows_for_class(df, cls, n=1000, seed=SEED+i)\n",
        "    pretrain_parts.append(samp)\n",
        "    used_idx.update(samp.index.tolist())\n",
        "\n",
        "df_pretrain = pd.concat(pretrain_parts).reset_index(drop=True)\n",
        "print(\"Pretrain rows:\", len(df_pretrain), \" expected:\", 10*1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JS5xIUfb3kC5"
      },
      "outputs": [],
      "source": [
        "# Cropping function where we convert bbox to patch (with optional context expansion)\n",
        "def crop_patch(\n",
        "    img: Image.Image,\n",
        "    x, y, w, h,\n",
        "    pad_to_square=True,\n",
        "    expand_factor=0.20,    # e.g., 0.20 means 20% of bbox size added on each side\n",
        "    expand_px=0           # optional fixed extra pixels on each side\n",
        "):\n",
        "    # Expand bbox to include context\n",
        "    dx = expand_px + expand_factor * float(w)\n",
        "    dy = expand_px + expand_factor * float(h)\n",
        "\n",
        "    x1 = int(round(x - dx))\n",
        "    y1 = int(round(y - dy))\n",
        "    x2 = int(round(x + w + dx))\n",
        "    y2 = int(round(y + h + dy))\n",
        "\n",
        "    # Clip to image boundaries\n",
        "    x1 = max(0, x1); y1 = max(0, y1)\n",
        "    x2 = min(img.width, x2); y2 = min(img.height, y2)\n",
        "\n",
        "    # Safety: avoid empty crops\n",
        "    if x2 <= x1: x2 = min(img.width, x1 + 1)\n",
        "    if y2 <= y1: y2 = min(img.height, y1 + 1)\n",
        "\n",
        "    patch = img.crop((x1, y1, x2, y2))\n",
        "\n",
        "    if not pad_to_square:\n",
        "        return patch\n",
        "\n",
        "    # Pad to square (keeps aspect ratio before resize)\n",
        "    side = max(patch.width, patch.height)\n",
        "    new_img = Image.new(\"L\", (side, side), color=0)  # grayscale canvas\n",
        "    px = (side - patch.width) // 2\n",
        "    py = (side - patch.height) // 2\n",
        "    patch = patch.convert(\"L\")\n",
        "    new_img.paste(patch, (px, py))\n",
        "    return new_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f0KG4zfs4EPi"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import torch\n",
        "\n",
        "class ColonyPatchDatasetCached(Dataset):\n",
        "    \"\"\"\n",
        "    REAL caching:\n",
        "    - Caches the final tensor (after crop + transform) in memory\n",
        "    - Reuses it on next epochs\n",
        "    \"\"\"\n",
        "    def __init__(self, df, images_dir, class_to_idx, transform=None, cache_in_ram=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.images_dir = images_dir\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "        self.cache_in_ram = cache_in_ram\n",
        "        self._cache = {}  # idx -> (tensor_x, tensor_y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def _resolve_img_path(self, row):\n",
        "        if \"image_path\" in self.df.columns:\n",
        "            p = str(row[\"image_path\"])\n",
        "            if os.path.exists(p):\n",
        "                return p\n",
        "\n",
        "        name = str(row[\"image_name\"])\n",
        "        p2 = os.path.join(self.images_dir, name)\n",
        "        if os.path.exists(p2):\n",
        "            return p2\n",
        "\n",
        "        p3 = os.path.join(self.images_dir, os.path.basename(name))\n",
        "        if os.path.exists(p3):\n",
        "            return p3\n",
        "\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not find image.\\n\"\n",
        "            f\"image_name={name}\\n\"\n",
        "            f\"image_path={row.get('image_path', None)}\\n\"\n",
        "            f\"tried={p2}\\ntried={p3}\\nimages_dir={self.images_dir}\"\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # ✅ return cached sample if available\n",
        "        if self.cache_in_ram and idx in self._cache:\n",
        "            return self._cache[idx]\n",
        "\n",
        "        r = self.df.iloc[idx]\n",
        "        img_path = self._resolve_img_path(r)\n",
        "\n",
        "        img = Image.open(img_path).convert(\"L\")\n",
        "\n",
        "        x = float(r[\"bbox_x\"]); y = float(r[\"bbox_y\"])\n",
        "        w = float(r[\"bbox_width\"]); h = float(r[\"bbox_height\"])\n",
        "\n",
        "        patch = crop_patch(img, x, y, w, h)  # must return fixed-size PIL patch\n",
        "        patch_rgb = Image.merge(\"RGB\", (patch, patch, patch))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            x_out = self.transform(patch_rgb)\n",
        "        else:\n",
        "            # if no transform, convert to tensor so batching works\n",
        "            x_out = torch.from_numpy(np.array(patch_rgb)).permute(2,0,1).float() / 255.0\n",
        "\n",
        "        if \"label_idx\" in self.df.columns:\n",
        "            y_out = torch.tensor(int(r[\"label_idx\"]), dtype=torch.long)\n",
        "        else:\n",
        "            y_out = torch.tensor(int(self.class_to_idx[str(r[\"label_name\"])]), dtype=torch.long)\n",
        "\n",
        "        if self.cache_in_ram:\n",
        "            self._cache[idx] = (x_out, y_out)\n",
        "\n",
        "        return x_out, y_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aVbqNKUX4T35"
      },
      "outputs": [],
      "source": [
        "# ImageNet normalization stats (because we use an ImageNet-pretrained backbone)\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
        "\n",
        "train_tf = T.Compose([\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomVerticalFlip(),\n",
        "    T.RandomRotation(10),\n",
        "    T.RandomResizedCrop(100, scale=(0.85, 1.0)),   # helps robustness\n",
        "    T.ColorJitter(brightness=0.2, contrast=0.2),   # important for new images\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "\n",
        "test_tf = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c2WwKm_Z4io7"
      },
      "outputs": [],
      "source": [
        "def accuracy(pred_logits, y):\n",
        "    preds = pred_logits.argmax(dim=1)\n",
        "    return (preds == y).float().mean().item()\n",
        "\n",
        "def train_one_epoch(model, loader, optim, criterion):\n",
        "    model.train()\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        bs = x.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy(logits, y) * bs\n",
        "        n += bs\n",
        "\n",
        "    return total_loss / n, total_acc / n\n",
        "\n",
        "@torch.no_grad()  # helps to stop gradient tracking during evaluation\n",
        "def eval_one_epoch(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        bs = x.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy(logits, y) * bs\n",
        "        n += bs\n",
        "\n",
        "    return total_loss / n, total_acc / n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIdWtwdS43PF",
        "outputId": "87ad060e-145f-4ffa-a9be-15eeb8ff7389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-class train: 8000 test: 2000\n"
          ]
        }
      ],
      "source": [
        "class_to_idx_10 = {c:i for i,c in enumerate(CLASSES_10)}\n",
        "\n",
        "train_df_10, test_df_10 = train_test_split(\n",
        "    df_pretrain,\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    stratify=df_pretrain[\"label_name\"]\n",
        ")\n",
        "\n",
        "ds_train_10 = ColonyPatchDatasetCached(train_df_10, IMAGES_DIR_PUBLIC, class_to_idx_10, transform=train_tf)\n",
        "ds_test_10  = ColonyPatchDatasetCached(test_df_10,  IMAGES_DIR_PUBLIC, class_to_idx_10, transform=test_tf)\n",
        "\n",
        "train_loader_10 = DataLoader(ds_train_10, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
        "test_loader_10  = DataLoader(ds_test_10,  batch_size=128, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "\n",
        "print(\"10-class train:\", len(ds_train_10), \"test:\", len(ds_test_10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x7bXe8S5547y"
      },
      "outputs": [],
      "source": [
        "#we are going to use the pretrained back bone for imagenet weights\n",
        "#ResNet-18 architecture suitable for efficient transfer learning.\n",
        "#Load the official pretrained ResNet18 weights trained on ImageNet-1K(1000 classes), version 1\n",
        "weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
        "model_10 = models.resnet18(weights=weights)\n",
        "\n",
        "# change the resnet classifier head from 1000 to 10 classes\n",
        "in_features = model_10.fc.in_features\n",
        "model_10.fc = nn.Linear(in_features, len(CLASSES_10))\n",
        "\n",
        "model_10 = model_10.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_10.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# FIX: Make all 10-class images same size (prevents stack error)\n",
        "# ============================\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1) Set a fixed size (ResNet standard)\n",
        "IMG_SIZE_10 = 224\n",
        "\n",
        "# 2) Define transforms that ALWAYS resize\n",
        "train_tf_10 = T.Compose([\n",
        "    T.Resize((IMG_SIZE_10, IMG_SIZE_10)),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "test_tf_10 = T.Compose([\n",
        "    T.Resize((IMG_SIZE_10, IMG_SIZE_10)),\n",
        "    T.ToTensor(),\n",
        "])\n",
        "\n",
        "# 3) Rebuild datasets using the SAME dataset objects you already created\n",
        "#    This works if your dataset has `.transform` field like torchvision ImageFolder\n",
        "assert \"ds_train_10\" in globals(), \"ds_train_10 not found. Find the cell where you created it.\"\n",
        "assert \"ds_test_10\" in globals(),  \"ds_test_10 not found. Find the cell where you created it.\"\n",
        "\n",
        "ds_train_10.transform = train_tf_10\n",
        "ds_test_10.transform  = test_tf_10\n",
        "\n",
        "# 4) Rebuild loaders (important!)\n",
        "BATCH_SIZE_10 = 64\n",
        "\n",
        "train_loader_10 = DataLoader(\n",
        "    ds_train_10, batch_size=BATCH_SIZE_10, shuffle=True,\n",
        "    num_workers=0, pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader_10 = DataLoader(\n",
        "    ds_test_10, batch_size=BATCH_SIZE_10, shuffle=False,\n",
        "    num_workers=0, pin_memory=True\n",
        ")\n",
        "\n",
        "# 5) Quick verification\n",
        "x, y = next(iter(test_loader_10))\n",
        "print(\"✅ test_loader_10 batch shape:\", x.shape)  # must be [B, 3, 224, 224]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeN2o205a2w9",
        "outputId": "d22d662a-4dbf-4808-c374-2db6fbb8381c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ test_loader_10 batch shape: torch.Size([64, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abTl3Ece6Iem",
        "outputId": "79c22c5c-0526-4844-fd2b-39e8e23bf7c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10-class] Epoch 01 | train acc 0.792 | test acc 0.875\n",
            "[10-class] Epoch 02 | train acc 0.933 | test acc 0.913\n",
            "[10-class] Epoch 03 | train acc 0.961 | test acc 0.923\n",
            "[10-class] Epoch 04 | train acc 0.971 | test acc 0.926\n",
            "[10-class] Epoch 05 | train acc 0.976 | test acc 0.904\n",
            "[10-class] Epoch 06 | train acc 0.977 | test acc 0.918\n",
            "[10-class] Epoch 07 | train acc 0.985 | test acc 0.916\n",
            "[10-class] Epoch 08 | train acc 0.990 | test acc 0.934\n",
            "[10-class] Epoch 09 | train acc 0.992 | test acc 0.920\n",
            "[10-class] Epoch 10 | train acc 0.992 | test acc 0.891\n",
            "Best 10-class test acc: 0.934\n"
          ]
        }
      ],
      "source": [
        "EPOCHS_10 = 10  # start small; you can increase later\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(1, EPOCHS_10+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model_10, train_loader_10, optimizer, criterion)\n",
        "    te_loss, te_acc = eval_one_epoch(model_10, test_loader_10, criterion)\n",
        "\n",
        "    print(f\"[10-class] Epoch {epoch:02d} | train acc {tr_acc:.3f} | test acc {te_acc:.3f}\")\n",
        "\n",
        "    #save the model with best accuracy\n",
        "    if te_acc > best_acc:\n",
        "        best_acc = te_acc\n",
        "        torch.save(model_10.state_dict(), os.path.join(WORKDIR_PRETRAIN, \"resnet18_pretrained_10class.pt\"))\n",
        "\n",
        "print(\"Best 10-class test acc:\", best_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, zipfile, yaml\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "def parse_cvat_yolo_zip_to_df(gt_zip_path, images_dir, seed=42, source_tag=\"\"):\n",
        "    GT_DIR = \"/content/_tmp_gt_extract\"\n",
        "    if os.path.exists(GT_DIR):\n",
        "        import shutil; shutil.rmtree(GT_DIR)\n",
        "    os.makedirs(GT_DIR, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(gt_zip_path, \"r\") as z:\n",
        "        z.extractall(GT_DIR)\n",
        "\n",
        "    yaml_path = os.path.join(GT_DIR, \"data.yaml\")\n",
        "    with open(yaml_path, \"r\") as f:\n",
        "        y = yaml.safe_load(f)\n",
        "\n",
        "    names = y[\"names\"]\n",
        "    if isinstance(names, dict):\n",
        "        id_to_class = {int(k): v for k, v in names.items()}\n",
        "    else:\n",
        "        id_to_class = {i: v for i, v in enumerate(names)}\n",
        "\n",
        "    classes = [id_to_class[i] for i in sorted(id_to_class.keys())]\n",
        "\n",
        "    labels_dir = os.path.join(GT_DIR, \"labels\", \"train\")\n",
        "    if not os.path.isdir(labels_dir):\n",
        "        labels_dir = os.path.join(GT_DIR, \"labels\")\n",
        "    if not os.path.isdir(labels_dir):\n",
        "        raise FileNotFoundError(\"Could not find labels folder inside GT zip.\")\n",
        "\n",
        "    def find_image_path(stem):\n",
        "        for ext in [\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".webp\"]:\n",
        "            p = os.path.join(images_dir, stem + ext)\n",
        "            if os.path.exists(p):\n",
        "                return p\n",
        "        return None\n",
        "\n",
        "    rows, missing = [], []\n",
        "    for txt_name in os.listdir(labels_dir):\n",
        "        if not txt_name.endswith(\".txt\"):\n",
        "            continue\n",
        "\n",
        "        stem = os.path.splitext(txt_name)[0]\n",
        "        img_path = find_image_path(stem)\n",
        "        if img_path is None:\n",
        "            missing.append(stem)\n",
        "            continue\n",
        "\n",
        "        img = Image.open(img_path)\n",
        "        W, H = img.size\n",
        "\n",
        "        with open(os.path.join(labels_dir, txt_name), \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) != 5:\n",
        "                    continue\n",
        "\n",
        "                cls_id = int(float(parts[0]))\n",
        "                cx, cy, w, h = map(float, parts[1:])\n",
        "\n",
        "                bw = w * W\n",
        "                bh = h * H\n",
        "                x1 = (cx * W) - bw/2\n",
        "                y1 = (cy * H) - bh/2\n",
        "\n",
        "                rows.append({\n",
        "                    \"label_name\": id_to_class[cls_id],\n",
        "                    \"bbox_x\": x1, \"bbox_y\": y1,\n",
        "                    \"bbox_width\": bw, \"bbox_height\": bh,\n",
        "                    \"image_name\": os.path.basename(img_path),\n",
        "                    \"image_path\": img_path,                 # ✅ FIX\n",
        "                    \"source\": source_tag,                   # ✅ helps debugging\n",
        "                    \"image_width\": W, \"image_height\": H\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    print(\"Parsed:\", gt_zip_path)\n",
        "    print(\"Boxes:\", len(df), \"Images:\", df[\"image_path\"].nunique())\n",
        "    print(\"Classes:\", classes)\n",
        "    if missing:\n",
        "        print(\"WARNING: Missing images for some labels (examples):\", missing[:10])\n",
        "\n",
        "    return df, classes\n"
      ],
      "metadata": {
        "id": "D3KLnOQlG9Ob"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ft, classes_ft   = parse_cvat_yolo_zip_to_df(FT_GT_ZIP_PATH, IMAGES_DIR_FT, seed=SEED, source_tag=\"old\")\n",
        "df_test, classes_ts = parse_cvat_yolo_zip_to_df(TEST_GT_ZIP_PATH, IMAGES_DIR_TEST, seed=SEED, source_tag=\"new\")\n",
        "\n",
        "# ✅ FIX: compare as sets (order doesn't matter)\n",
        "if set(classes_ft) != set(classes_ts):\n",
        "    raise ValueError(\n",
        "        f\"Class mismatch!\\nTrain classes: {classes_ft}\\nTest classes:  {classes_ts}\"\n",
        "    )\n",
        "\n",
        "VALID_CLASSES = [\"sp01\", \"sp02\", \"sp03\", \"sp04\"]  # keep these classes\n",
        "\n",
        "df_ft   = df_ft[df_ft[\"label_name\"].isin(VALID_CLASSES)].reset_index(drop=True)\n",
        "df_test = df_test[df_test[\"label_name\"].isin(VALID_CLASSES)].reset_index(drop=True)\n",
        "\n",
        "df_all = pd.concat([df_ft, df_test], ignore_index=True)\n",
        "df_all = df_all.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "# ✅ Canonical order (stable)\n",
        "CLASSES_FT = sorted(df_all[\"label_name\"].unique().tolist())\n",
        "class_to_idx_ft = {c:i for i,c in enumerate(CLASSES_FT)}\n",
        "\n",
        "print(\"Using classes:\", CLASSES_FT)\n",
        "print(\"Class → idx mapping:\", class_to_idx_ft)\n",
        "print(\"Class counts:\\n\", df_all[\"label_name\"].value_counts())\n",
        "print(\"Unique images per class:\\n\", df_all.groupby(\"label_name\")[\"image_path\"].nunique())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaVih4R7M7sv",
        "outputId": "58a0ade3-7a5b-4216-cb45-5fb441553c3b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed: /content/drive/MyDrive/22022540/ground_truth/ground_truth_one/ground_truth_dataset.zip\n",
            "Boxes: 254 Images: 4\n",
            "Classes: ['sp01', 'sp02', 'sp03', 'sp04']\n",
            "Parsed: /content/drive/MyDrive/22022540/ground_truth/ground_truth_updated/updated_groud_truth.zip\n",
            "Boxes: 265 Images: 4\n",
            "Classes: ['sp01', 'sp03', 'sp04', 'sp02']\n",
            "Using classes: ['sp01', 'sp02', 'sp03', 'sp04']\n",
            "Class → idx mapping: {'sp01': 0, 'sp02': 1, 'sp03': 2, 'sp04': 3}\n",
            "Class counts:\n",
            " label_name\n",
            "sp02    230\n",
            "sp01    100\n",
            "sp04    100\n",
            "sp03     89\n",
            "Name: count, dtype: int64\n",
            "Unique images per class:\n",
            " label_name\n",
            "sp01    2\n",
            "sp02    2\n",
            "sp03    2\n",
            "sp04    2\n",
            "Name: image_path, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Old set boxes:\", len(df_ft))\n",
        "print(\"New set boxes:\", len(df_test))\n",
        "\n",
        "# 1) MIX old + new\n",
        "df_all = pd.concat([df_ft, df_test], ignore_index=True)\n",
        "\n",
        "# (Optional) shuffle once for safety\n",
        "df_all = df_all.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nMixed boxes total:\", len(df_all))\n",
        "print(\"Class distribution (mixed):\")\n",
        "print(df_all[\"label_name\"].value_counts())\n",
        "\n",
        "# 2) 80/20 split (stratified by class)\n",
        "try:\n",
        "    train_df_ft, test_df_new = train_test_split(\n",
        "        df_all,\n",
        "        test_size=0.2,\n",
        "        random_state=SEED,\n",
        "        stratify=df_all[\"label_name\"]\n",
        "    )\n",
        "    print(\"\\n✅ Using STRATIFIED 80/20 split on mixed data\")\n",
        "except ValueError as e:\n",
        "    print(\"\\n⚠️ Stratified split failed (some class too small). Using random split.\")\n",
        "    train_df_ft, test_df_new = train_test_split(\n",
        "        df_all,\n",
        "        test_size=0.2,\n",
        "        random_state=SEED,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "print(\"Train (80%):\", len(train_df_ft))\n",
        "print(\"Test  (20%):\", len(test_df_new))\n",
        "print(\"\\nTrain class counts:\")\n",
        "print(train_df_ft[\"label_name\"].value_counts())\n",
        "print(\"\\nTest class counts:\")\n",
        "print(test_df_new[\"label_name\"].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MboirmfUgx88",
        "outputId": "61f7f0d6-fd5b-40a9-a2d3-e94a6ec918cc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old set boxes: 254\n",
            "New set boxes: 265\n",
            "\n",
            "Mixed boxes total: 519\n",
            "Class distribution (mixed):\n",
            "label_name\n",
            "sp02    230\n",
            "sp01    100\n",
            "sp04    100\n",
            "sp03     89\n",
            "Name: count, dtype: int64\n",
            "\n",
            "✅ Using STRATIFIED 80/20 split on mixed data\n",
            "Train (80%): 415\n",
            "Test  (20%): 104\n",
            "\n",
            "Train class counts:\n",
            "label_name\n",
            "sp02    184\n",
            "sp04     80\n",
            "sp01     80\n",
            "sp03     71\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test class counts:\n",
            "label_name\n",
            "sp02    46\n",
            "sp01    20\n",
            "sp04    20\n",
            "sp03    18\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# FIX: Always return same-size patches so DataLoader can stack batches\n",
        "# ============================\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# --- REQUIRED: PATCH_SIZE must exist (or default) ---\n",
        "PATCH_SIZE = globals().get(\"PATCH_SIZE\", 100)\n",
        "\n",
        "class ColonyPatchDatasetFixed(Dataset):\n",
        "    \"\"\"\n",
        "    Fixes the batching crash:\n",
        "    - Resolves image path robustly (image_path -> images_dir/name -> basename fallback)\n",
        "    - Crops patch using crop_patch()\n",
        "    - FORCES output patch to PATCH_SIZE x PATCH_SIZE (always)\n",
        "    - Converts L -> RGB\n",
        "    - Applies transform (if transform already includes resize, that's fine too)\n",
        "    \"\"\"\n",
        "    def __init__(self, df, images_dir, class_to_idx, transform=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.images_dir = images_dir\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def _resolve_img_path(self, row):\n",
        "        if \"image_path\" in self.df.columns:\n",
        "            p = str(row[\"image_path\"])\n",
        "            if os.path.exists(p):\n",
        "                return p\n",
        "\n",
        "        name = str(row[\"image_name\"])\n",
        "        p2 = os.path.join(self.images_dir, name)\n",
        "        if os.path.exists(p2):\n",
        "            return p2\n",
        "\n",
        "        p3 = os.path.join(self.images_dir, os.path.basename(name))\n",
        "        if os.path.exists(p3):\n",
        "            return p3\n",
        "\n",
        "        raise FileNotFoundError(\n",
        "            f\"Could not find image:\\n\"\n",
        "            f\"  image_name: {name}\\n\"\n",
        "            f\"  image_path col: {row.get('image_path', None)}\\n\"\n",
        "            f\"  tried: {p2}\\n\"\n",
        "            f\"  tried: {p3}\\n\"\n",
        "            f\"  images_dir: {self.images_dir}\"\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.df.iloc[idx]\n",
        "        img_path = self._resolve_img_path(r)\n",
        "\n",
        "        # IMPORTANT: keep same as your pipeline (grayscale first)\n",
        "        img_L = Image.open(img_path).convert(\"L\")\n",
        "\n",
        "        x = float(r[\"bbox_x\"]); y = float(r[\"bbox_y\"])\n",
        "        w = float(r[\"bbox_width\"]); h = float(r[\"bbox_height\"])\n",
        "\n",
        "        # ---- crop patch (whatever size it returns) ----\n",
        "        patch_L = crop_patch(img_L, x, y, w, h)\n",
        "\n",
        "        # ---- FORCE fixed size (this is the key fix) ----\n",
        "        if patch_L.size != (PATCH_SIZE, PATCH_SIZE):\n",
        "            patch_L = patch_L.resize((PATCH_SIZE, PATCH_SIZE))\n",
        "\n",
        "        # L -> RGB\n",
        "        patch_rgb = Image.merge(\"RGB\", (patch_L, patch_L, patch_L))\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform is not None:\n",
        "            patch_rgb = self.transform(patch_rgb)\n",
        "\n",
        "        # Label\n",
        "        if \"label_idx\" in self.df.columns:\n",
        "            y_idx = int(r[\"label_idx\"])\n",
        "        else:\n",
        "            y_idx = int(self.class_to_idx[str(r[\"label_name\"])])\n",
        "\n",
        "        return patch_rgb, torch.tensor(y_idx, dtype=torch.long)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GhM-NlqpfiS6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Rebuild datasets + loaders (same variable names as your training code)\n",
        "# ============================\n",
        "class_to_idx_ft = {c:i for i,c in enumerate(CLASSES_FT)}\n",
        "\n",
        "ds_train_ft = ColonyPatchDatasetFixed(train_df_ft, IMAGES_DIR_FT, class_to_idx_ft, transform=train_tf)\n",
        "ds_test_new = ColonyPatchDatasetFixed(test_df_new, IMAGES_DIR_TEST, class_to_idx_ft, transform=test_tf)\n",
        "\n",
        "train_loader_ft = DataLoader(ds_train_ft, batch_size=64, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader_new = DataLoader(ds_test_new, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"✅ Train patches (A):\", len(ds_train_ft))\n",
        "print(\"✅ Test  patches (B):\", len(ds_test_new))\n",
        "\n"
      ],
      "metadata": {
        "id": "pWxG_FUHhVUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecee4350-79cc-4993-a60d-eb5c99d2b736"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Train patches (A): 415\n",
            "✅ Test  patches (B): 104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Model load (10-class -> replace head to FT classes)\n",
        "# ============================\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "ckpt_10 = os.path.join(WORKDIR_PRETRAIN, \"resnet18_pretrained_10class.pt\")\n",
        "\n",
        "model = models.resnet18(weights=None)\n",
        "in_features = model.fc.in_features\n",
        "\n",
        "# load 10-class checkpoint\n",
        "model.fc = nn.Linear(in_features, 10)\n",
        "model.load_state_dict(torch.load(ckpt_10, map_location=\"cpu\"))\n",
        "\n",
        "# replace head for in-house fine-tune\n",
        "model.fc = nn.Linear(in_features, len(CLASSES_FT))\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n"
      ],
      "metadata": {
        "id": "K2gZ9ZqHhwqk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory to save fine-tuned models\n",
        "WORKDIR = \"/content/work_finetune\"\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "\n",
        "print(\"Fine-tuning models will be saved to:\", WORKDIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W15DxPu7jNUA",
        "outputId": "4ef837a8-3e5b-41fb-9e35-e0634eb85127"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning models will be saved to: /content/work_finetune\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# ============================\n",
        "# Training (your exact loop)\n",
        "# ============================\n",
        "best_path = os.path.join(WORKDIR, \"best_finetuned_on_newtest.pt\")\n",
        "best_acc = 0.0\n",
        "\n",
        "def maybe_save_best(model, acc, best_acc, path):\n",
        "    if acc > best_acc:\n",
        "        torch.save(model.state_dict(), path)\n",
        "        return acc, True\n",
        "    return best_acc, False\n",
        "\n",
        "print(\"\\n===== PHASE 1: Head-only (features frozen) | Train=A (100%) → Test=B =====\")\n",
        "\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "for p in model.fc.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "optimizer1 = torch.optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer1, mode=\"max\", factor=0.5, patience=2, threshold=1e-3, min_lr=1e-6\n",
        ")\n",
        "\n",
        "EPOCHS_HEAD = 10\n",
        "for epoch in range(1, EPOCHS_HEAD + 1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model, train_loader_ft, optimizer1, criterion)\n",
        "    te_loss, te_acc = eval_one_epoch(model, test_loader_new, criterion)\n",
        "\n",
        "    scheduler1.step(te_acc)\n",
        "    lr1 = optimizer1.param_groups[0][\"lr\"]\n",
        "\n",
        "    print(f\"[HEAD] Epoch {epoch:02d} | lr {lr1:.2e} | train acc(A) {tr_acc:.3f} | test acc(B) {te_acc:.3f}\")\n",
        "    best_acc, saved = maybe_save_best(model, te_acc, best_acc, best_path)\n",
        "\n",
        "print(\"Best test(B) acc after Phase 1:\", best_acc)\n",
        "\n",
        "print(\"\\n===== PHASE 2: Full fine-tune (unfreeze all) | Train=A (100%) → Test=B =====\")\n",
        "\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "optimizer2 = torch.optim.Adam(model.parameters(), lr=3e-5, weight_decay=1e-4)\n",
        "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer2, mode=\"max\", factor=0.5, patience=2, threshold=1e-3, min_lr=1e-6\n",
        ")\n",
        "\n",
        "EPOCHS_FULL = 10\n",
        "for epoch in range(1, EPOCHS_FULL + 1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model, train_loader_ft, optimizer2, criterion)\n",
        "    te_loss, te_acc = eval_one_epoch(model, test_loader_new, criterion)\n",
        "\n",
        "    scheduler2.step(te_acc)\n",
        "    lr2 = optimizer2.param_groups[0][\"lr\"]\n",
        "\n",
        "    print(f\"[FULL] Epoch {epoch:02d} | lr {lr2:.2e} | train acc(A) {tr_acc:.3f} | test acc(B) {te_acc:.3f}\")\n",
        "    best_acc, saved = maybe_save_best(model, te_acc, best_acc, best_path)\n",
        "\n",
        "print(\"\\n✅ FINAL Best test accuracy on NEW dataset (B):\", best_acc)\n",
        "print(\"✅ Best model saved at:\", best_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pasHmeSdyet",
        "outputId": "7237d36e-6013-430a-ec22-3d8b322b37f7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== PHASE 1: Head-only (features frozen) | Train=A (100%) → Test=B =====\n",
            "[HEAD] Epoch 01 | lr 1.00e-03 | train acc(A) 0.390 | test acc(B) 0.462\n",
            "[HEAD] Epoch 02 | lr 1.00e-03 | train acc(A) 0.499 | test acc(B) 0.558\n",
            "[HEAD] Epoch 03 | lr 1.00e-03 | train acc(A) 0.545 | test acc(B) 0.587\n",
            "[HEAD] Epoch 04 | lr 1.00e-03 | train acc(A) 0.593 | test acc(B) 0.596\n",
            "[HEAD] Epoch 05 | lr 1.00e-03 | train acc(A) 0.593 | test acc(B) 0.615\n",
            "[HEAD] Epoch 06 | lr 1.00e-03 | train acc(A) 0.614 | test acc(B) 0.635\n",
            "[HEAD] Epoch 07 | lr 1.00e-03 | train acc(A) 0.634 | test acc(B) 0.635\n",
            "[HEAD] Epoch 08 | lr 1.00e-03 | train acc(A) 0.614 | test acc(B) 0.644\n",
            "[HEAD] Epoch 09 | lr 1.00e-03 | train acc(A) 0.684 | test acc(B) 0.644\n",
            "[HEAD] Epoch 10 | lr 1.00e-03 | train acc(A) 0.680 | test acc(B) 0.673\n",
            "Best test(B) acc after Phase 1: 0.6730769184919504\n",
            "\n",
            "===== PHASE 2: Full fine-tune (unfreeze all) | Train=A (100%) → Test=B =====\n",
            "[FULL] Epoch 01 | lr 3.00e-05 | train acc(A) 0.694 | test acc(B) 0.702\n",
            "[FULL] Epoch 02 | lr 3.00e-05 | train acc(A) 0.783 | test acc(B) 0.817\n",
            "[FULL] Epoch 03 | lr 3.00e-05 | train acc(A) 0.805 | test acc(B) 0.808\n",
            "[FULL] Epoch 04 | lr 3.00e-05 | train acc(A) 0.836 | test acc(B) 0.846\n",
            "[FULL] Epoch 05 | lr 3.00e-05 | train acc(A) 0.882 | test acc(B) 0.856\n",
            "[FULL] Epoch 06 | lr 3.00e-05 | train acc(A) 0.877 | test acc(B) 0.827\n",
            "[FULL] Epoch 07 | lr 3.00e-05 | train acc(A) 0.875 | test acc(B) 0.837\n",
            "[FULL] Epoch 08 | lr 1.50e-05 | train acc(A) 0.901 | test acc(B) 0.837\n",
            "[FULL] Epoch 09 | lr 1.50e-05 | train acc(A) 0.920 | test acc(B) 0.846\n",
            "[FULL] Epoch 10 | lr 1.50e-05 | train acc(A) 0.930 | test acc(B) 0.856\n",
            "\n",
            "✅ FINAL Best test accuracy on NEW dataset (B): 0.8557692445241488\n",
            "✅ Best model saved at: /content/work_finetune/best_finetuned_on_newtest.pt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyNDovJkeTxJFxG9ByLTunGk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}